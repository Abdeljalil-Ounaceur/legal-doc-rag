# -*- coding: utf-8 -*-
"""Legal Document Question Answering using RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1icpN2wNlE1-IojADOGAySjWOAiSWjNzL

# Legal Document Question Answering using RAG (DistilBERT + FAISS)
"""

# Install required packages
#!pip install transformers datasets sentence-transformers faiss-cpu PyMuPDF !pip install gradio -q

from transformers import pipeline
from sentence_transformers import SentenceTransformer
import faiss
import fitz  # PyMuPDF
import numpy as np

# Replace with your law PDF path
pdf_path = "google_terms_of_service_en.pdf"

doc = fitz.open(pdf_path)
full_text = ""
for page in doc:
    full_text += page.get_text("text")

# Try semantic chunking instead
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " ", ""]
)
chunks = splitter.split_text(full_text)

print("\n\n\n\n\n\n".join(chunks[:5]))

embedder = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = embedder.encode(chunks, convert_to_tensor=False)

dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

from transformers import pipeline

# Better generative model that can synthesize answers
qa_model = pipeline("text2text-generation", model="google/flan-t5-large")  # Larger model

def answer_question(question, num_chunks=16):
    # Find relevant chunks
    q_embedding = embedder.encode([question])
    distances, chunk_ids = index.search(np.array(q_embedding), num_chunks)

    # Just grab the top chunks - keep it simple
    context = ""
    for chunk_id in chunk_ids[0]:
        context += chunks[chunk_id] + " "

    # Create a clear prompt for the generative model
    prompt = f"""Based on the context below, provide a complete and helpful answer. If you need to synthesize information from multiple parts, do so. If the answer isn't clearly covered, say 'not mentioned'.

Context: {context}

Question: {question}

Provide a clear, comprehensive answer:"""

    # Generate answer
    result = qa_model(prompt, max_length=200, do_sample=False)
    return result[0]['generated_text']



import gradio as gr

def qa_interface(question):
    if question.strip():
        return answer_question(question)
    return "Please ask a question."

# Create Gradio interface
interface = gr.Interface(
    fn=qa_interface,
    inputs=gr.Textbox(label="Ask about Google's Terms of Service", placeholder="What are the age requirements?"),
    outputs=gr.Textbox(label="Answer"),
    title="Legal Document QA (RAG)",
    description="Ask questions about Google's Terms of Service using RAG (Retrieval Augmented Generation)"
)

# Launch with public link
interface.launch(share=True)

